{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import operator\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "We load a total of 6 data files, 2 random samples of 5K, 10K, 15K from a dataset of the reviews that have more than 40 characters of the top 100 most-reviewed-businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', errors='ignore' ) as json_file: \n",
    "        data = pd.read_json(json_file)\n",
    "    data = data.reset_index()\n",
    "    data['text_len'] = [len(t) for t in data['text']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 6 datasets of reviews: 2 random samples of review count 50K, 100K, 500K\n",
    "data_1 = load_data('./files/2sample_5000_1.json')\n",
    "data_2 = load_data('./files/2sample_5000_2.json')\n",
    "data_3 = load_data('./files/2sample_10000_1.json')\n",
    "data_4 = load_data('./files/2sample_10000_2.json')\n",
    "data_5 = load_data('./files/2sample_15000_1.json')\n",
    "data_6 = load_data('./files/2sample_15000_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1_reviews = data_1.loc[:,['text']]\n",
    "data_2_reviews = data_2.loc[:,['text']]\n",
    "data_3_reviews = data_3.loc[:,['text']]\n",
    "data_4_reviews = data_4.loc[:,['text']]\n",
    "data_5_reviews = data_5.loc[:,['text']]\n",
    "data_6_reviews = data_6.loc[:,['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1_reviews['tokenized_text'] = data_1_reviews['text'].apply(tokenize.sent_tokenize)\n",
    "data_2_reviews['tokenized_text'] = data_2_reviews['text'].apply(tokenize.sent_tokenize) \n",
    "data_3_reviews['tokenized_text'] = data_3_reviews['text'].apply(tokenize.sent_tokenize) \n",
    "data_4_reviews['tokenized_text'] = data_4_reviews['text'].apply(tokenize.sent_tokenize) \n",
    "data_5_reviews['tokenized_text'] = data_5_reviews['text'].apply(tokenize.sent_tokenize) \n",
    "data_6_reviews['tokenized_text'] = data_6_reviews['text'].apply(tokenize.sent_tokenize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use Out-of-box sentiment analyzers to find sentiment of review text\n",
    "\n",
    "We used VADER form the NLTK library and TextBlob.\n",
    "I attempted to use StanfordCoreNLP, but decided not to use it because it took too long to run. The sentiment tool in StanfordCoreNLP provided scores per sentence, which would be a comporable package especially to VADER, as the VADER documentation suggests to first tokenize text into sentences and use the sentiment analyzer per sentence. \n",
    "Ref: https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py\n",
    "We found the mean of sentiment scores from each sentence of a review using both the VADER and TextBlob package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds mean sentiment score of a list of sentences\n",
    "def avg_sentence_sentiment(sentence_list, analyzer):\n",
    "    review_sentiments = 0.0\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        if (analyzer == 'vader'):\n",
    "            vs = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "            review_sentiments += vs[\"compound\"]\n",
    "        elif (analyzer == 'textblob'):\n",
    "            t = TextBlob(sentence)\n",
    "            review_sentiments += t.sentiment[0]\n",
    "    return (review_sentiments / len(sentence_list))\n",
    "\n",
    "# function that adds a column each for the sentiment score for the vader and textblob package\n",
    "def find_sentiment(dataset):\n",
    "    dataset['vader_tok'] = dataset['tokenized_text'].apply(lambda x: avg_sentence_sentiment(x,'vader')) \n",
    "    dataset['textblob_tok'] = dataset['tokenized_text'].apply(lambda x: avg_sentence_sentiment(x,'textblob')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sentiments for all data samples\n",
    "find_sentiment(data_1_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sentiment(data_2_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sentiment(data_3_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sentiment(data_4_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sentiment(data_5_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sentiment(data_6_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the stars column onto the sentiment dataframe and save as a CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stars_sent_1 = pd.DataFrame(data = data_1.loc[:,'stars'], columns=['stars'])\n",
    "df_stars_sent_2 = pd.DataFrame(data = data_2.loc[:,'stars'], columns=['stars'])\n",
    "df_stars_sent_3 = pd.DataFrame(data = data_3.loc[:,'stars'], columns=['stars'])\n",
    "df_stars_sent_4 = pd.DataFrame(data = data_4.loc[:,'stars'], columns=['stars'])\n",
    "df_stars_sent_5 = pd.DataFrame(data = data_3.loc[:,'stars'], columns=['stars'])\n",
    "df_stars_sent_6 = pd.DataFrame(data = data_4.loc[:,'stars'], columns=['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stars_sent_1 = df_stars_sent_1.join(data_1_reviews)\n",
    "df_stars_sent_2 = df_stars_sent_2.join(data_2_reviews)\n",
    "df_stars_sent_3 = df_stars_sent_3.join(data_3_reviews)\n",
    "df_stars_sent_4 = df_stars_sent_4.join(data_4_reviews)\n",
    "df_stars_sent_5 = df_stars_sent_5.join(data_5_reviews)\n",
    "df_stars_sent_6 = df_stars_sent_6.join(data_6_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stars_sent_1.to_csv(\"./files/data_1.csv\")\n",
    "df_stars_sent_2.to_csv(\"./files/data_2.csv\")\n",
    "df_stars_sent_3.to_csv(\"./files/data_3.csv\")\n",
    "df_stars_sent_4.to_csv(\"./files/data_4.csv\")\n",
    "df_stars_sent_5.to_csv(\"./files/data_5.csv\")\n",
    "df_stars_sent_5.to_csv(\"./files/data_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
